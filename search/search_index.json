{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#eca-editor-code-assistant","title":"ECA (Editor Code Assistant)","text":"<p>Demo using eca-emacs </p> <p>Demo using eca-vscode </p> <p> installation \u2022   features \u2022   configuration \u2022   models \u2022   protocol </p> <ul> <li> Editor-agnostic: protocol for any editor to integrate.</li> <li> Single configuration: Configure eca making it work the same in any editor via global or local configs.</li> <li> Chat interface: ask questions, review code, work together to code.</li> <li> Agentic: let LLM work as an agent with its native tools and MCPs you can configure.</li> <li> Context: support: giving more details about your code to the LLM.</li> <li> Multi models: OpenAI, Anthropic, Ollama local models, and custom user config models.</li> </ul>"},{"location":"#rationale","title":"Rationale","text":"<p>A Free and OpenSource editor-agnostic tool that aims to easily link LLMs &lt;-&gt; Editors, giving the best UX possible for AI pair programming using a well-defined protocol. The server is written in Clojure and heavily inspired by the LSP protocol which is a success case for this kind of integration.</p> <p>The protocol makes easier for other editors integrate and having a server in the middle helps adding more features quickly like exporting metrics of features usage or single way to configure it for any editor.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>Install the plugin for your editor and ECA server will be downloaded and started automatically.</p>"},{"location":"#supported-editors","title":"Supported editors","text":"<ul> <li>Emacs</li> <li>VsCode</li> <li>Vim</li> <li>Intellij: Planned, help welcome</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":"<p>Editors spawn the server via <code>eca server</code> and communicate via stdin/stdout. Logs are printed to stderr, use <code>--verbose</code> to log client&lt;-&gt;server communication or <code>--log-level debug</code>  to log more info like LLM responses.</p> <p>Supported editors already download latest server on start and require no extra configuration</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Check the planned work here.</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>You can start eca with <code>--log-level debug</code> which should log helpful information in stderr buffer like what is being sent to LLMs.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome, please open an issue for discussion or a pull request. For developer details, check this doc.</p>"},{"location":"#support-the-project","title":"Support the project \ud83d\udc96","text":"<p>Consider sponsoring the project to help grow faster, the support helps to keep the project going, being updated and maintained!</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#0142","title":"0.14.2","text":"<ul> <li>Fix MCPs not starting because of graal reflection issue.</li> </ul>"},{"location":"CHANGELOG/#0141","title":"0.14.1","text":"<ul> <li>Fix native image build.</li> </ul>"},{"location":"CHANGELOG/#0140","title":"0.14.0","text":"<ul> <li>Support enable/disable tool servers.</li> <li>Bump mcp java sdk to 0.11.0.</li> </ul>"},{"location":"CHANGELOG/#0131","title":"0.13.1","text":"<ul> <li>Improve ollama model listing getting capabilities, avoiding change ollama config for different models.</li> </ul>"},{"location":"CHANGELOG/#0130","title":"0.13.0","text":"<ul> <li>Support reasoning for ollama models that support think.</li> </ul>"},{"location":"CHANGELOG/#0127","title":"0.12.7","text":"<ul> <li>Fix ollama tool calls.</li> </ul>"},{"location":"CHANGELOG/#0126","title":"0.12.6","text":"<ul> <li>fix web-search support for custom providers.</li> <li>fix output of eca_shell_command.</li> </ul>"},{"location":"CHANGELOG/#0125","title":"0.12.5","text":"<ul> <li>Improve tool call result marking as error when not expected output.</li> <li>Fix cases when tool calls output nothing.</li> </ul>"},{"location":"CHANGELOG/#0124","title":"0.12.4","text":"<ul> <li>Add chat command type.</li> </ul>"},{"location":"CHANGELOG/#0123","title":"0.12.3","text":"<ul> <li>Fix MCP prompts for anthropic models.</li> </ul>"},{"location":"CHANGELOG/#0122","title":"0.12.2","text":"<ul> <li>Fix tool calls</li> </ul>"},{"location":"CHANGELOG/#0121","title":"0.12.1","text":"<ul> <li>Improve welcome message.</li> </ul>"},{"location":"CHANGELOG/#0120","title":"0.12.0","text":"<ul> <li>Fix openai api key read from config.</li> <li>Support commands via <code>/</code>.</li> <li>Support MCP prompts via commands.</li> </ul>"},{"location":"CHANGELOG/#0112","title":"0.11.2","text":"<ul> <li>Fix error field on tool call outputs.</li> </ul>"},{"location":"CHANGELOG/#0111","title":"0.11.1","text":"<ul> <li>Fix reasoning for openai o models.</li> </ul>"},{"location":"CHANGELOG/#0110","title":"0.11.0","text":"<ul> <li>Add support for file contexts with line ranges.</li> </ul>"},{"location":"CHANGELOG/#0103","title":"0.10.3","text":"<ul> <li>Fix openai <code>max_output_tokens</code> message.</li> </ul>"},{"location":"CHANGELOG/#0102","title":"0.10.2","text":"<ul> <li>Fix usage metrics for anthropic models. </li> </ul>"},{"location":"CHANGELOG/#0101","title":"0.10.1","text":"<ul> <li>Improve <code>eca_read_file</code> tool to have better and more assertive descriptions/parameters.</li> </ul>"},{"location":"CHANGELOG/#0100","title":"0.10.0","text":"<ul> <li>Increase anthropic models maxTokens to 8196</li> <li>Support thinking/reasoning on models that support it.</li> </ul>"},{"location":"CHANGELOG/#090","title":"0.9.0","text":"<ul> <li>Include eca as a  server with tools.</li> <li>Support disable tools via config.</li> <li>Improve ECA prompt to be more precise and output with better quality</li> </ul>"},{"location":"CHANGELOG/#081","title":"0.8.1","text":"<ul> <li>Make generic tool server updates for eca native tools.</li> </ul>"},{"location":"CHANGELOG/#080","title":"0.8.0","text":"<ul> <li>Support tool call approval and configuration to manual approval.</li> <li>Initial support for repo-map context.</li> </ul>"},{"location":"CHANGELOG/#070","title":"0.7.0","text":"<ul> <li>Add client request to delete a chat.</li> </ul>"},{"location":"CHANGELOG/#061","title":"0.6.1","text":"<ul> <li>Support defaultModel in custom providers.</li> </ul>"},{"location":"CHANGELOG/#060","title":"0.6.0","text":"<ul> <li>Add usage tokens + cost to chat messages.</li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>Fix openai key</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>Support custom LLM providers via config.</li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li>Improve context query performance.</li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>Fix output of errored tool calls.</li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li>Fix arguments test when preparing tool call.</li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>Add support for global rules.</li> <li>Fix origin field of tool calls.</li> <li>Allow chat communication with no workspace opened.</li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Improve default model logic to check for configs and env vars of known models.</li> <li>Fix past messages sent to LLMs.</li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Support stop chat prompts via <code>chat/promptStop</code> notification.</li> <li>Fix anthropic messages history.</li> </ul>"},{"location":"CHANGELOG/#020","title":"0.2.0","text":"<ul> <li>Add native tools: filesystem</li> <li>Add MCP/tool support for ollama models.</li> <li>Improve ollama integration only requiring <code>ollama serve</code> to be running.</li> <li>Improve chat history and context passed to all LLM providers.</li> <li>Add support for prompt caching for Anthropic models.</li> </ul>"},{"location":"CHANGELOG/#010","title":"0.1.0","text":"<ul> <li>Allow comments on <code>json</code> configs.</li> <li>Improve MCP tool call feedback.</li> <li>Add support for env vars in mcp configs.</li> <li>Add <code>mcp/serverUpdated</code> server notification.</li> </ul>"},{"location":"CHANGELOG/#004","title":"0.0.4","text":"<ul> <li>Add env support for MCPs</li> <li>Add web_search capability</li> <li>Add <code>o3</code> model support.</li> <li>Support custom API urls for OpenAI and Anthropic</li> <li>Add <code>--log-level &lt;level&gt;</code> option for better debugging.</li> <li>Add support for global config file.</li> <li>Improve MCP response handling.</li> <li>Improve LLM streaming response handler.</li> </ul>"},{"location":"CHANGELOG/#003","title":"0.0.3","text":"<ul> <li>Fix ollama servers discovery</li> <li>Fix <code>.eca/config.json</code> read from workspace root</li> <li>Add support for MCP servers</li> </ul>"},{"location":"CHANGELOG/#002","title":"0.0.2","text":"<ul> <li>First alpha release</li> </ul>"},{"location":"CHANGELOG/#001","title":"0.0.1","text":""},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#ways-to-configure","title":"Ways to configure","text":"<p>Check all available configs here. There are 3 ways to configure ECA following this order of priority:</p>"},{"location":"configuration/#initializationoptions-convenient-for-editors","title":"InitializationOptions (convenient for editors)","text":"<p>Client editors can pass custom settings when sending the <code>initialize</code> request via the <code>initializationOptions</code> object:</p> <pre><code>\"initializationOptions\": {\n  \"chatBehavior\": \"chat\"\n}\n</code></pre>"},{"location":"configuration/#local-config-file-convenient-for-users","title":"Local Config file (convenient for users)","text":"<p><code>.eca/config.json</code> <pre><code>{\n  \"chatBehavior\": \"chat\"\n}\n</code></pre></p>"},{"location":"configuration/#global-config-file-convenient-for-users-and-multiple-projects","title":"Global config file (convenient for users and multiple projects)","text":"<p><code>~/.config/eca/config.json</code> <pre><code>{\n  \"chatBehavior\": \"chat\"\n}\n</code></pre></p>"},{"location":"configuration/#env-var","title":"Env Var","text":"<p>Via env var during server process spawn:</p> <pre><code>ECA_CONFIG='{\"myConfig\": \"my_value\"}' eca server\n</code></pre>"},{"location":"configuration/#rules","title":"Rules","text":"<p>Rules are contexts that are passed to the LLM during a prompt and are useful to tune prompts or LLM behavior. Rules are Multi-Document context files (<code>.mdc</code>) and the following metadata is supported:</p> <ul> <li><code>description</code>: a description used by LLM to decide whether to include this rule in context, absent means always include this rule.</li> <li><code>globs</code>: list of globs separated by <code>,</code>. When present the rule will be applied only when files mentioned matches those globs.</li> </ul> <p>There are 3 possible ways to configure rules following this order of priority:</p>"},{"location":"configuration/#project-file","title":"Project file","text":"<p>A <code>.eca/rules</code> folder from the workspace root containing <code>.mdc</code> files with the rules.</p> <p><code>.eca/rules/talk_funny.mdc</code> <pre><code>--- \ndescription: Use when responding anything\n---\n\n- Talk funny like Mickey!\n</code></pre></p>"},{"location":"configuration/#global-file","title":"Global file","text":"<p>A <code>$XDG_CONFIG_HOME/eca/rules</code> or <code>~/.config/eca/rules</code> folder containing <code>.mdc</code> files with the rules.</p> <p><code>~/.config/eca/rules/talk_funny.mdc</code> <pre><code>--- \ndescription: Use when responding anything\n---\n\n- Talk funny like Mickey!\n</code></pre></p>"},{"location":"configuration/#config","title":"Config","text":"<p>Just add to your config the <code>:rules</code> pointing to <code>.mdc</code> files that will be searched from the workspace root if not an absolute path:</p> <pre><code>{\n  \"rules\": [{\"path\": \"my-rule.mdc\"}]\n}\n</code></pre>"},{"location":"configuration/#mcp","title":"MCP","text":"<p>For MCP servers configuration, use the <code>mcpServers</code> config, example:</p> <p><code>.eca/config.json</code> <pre><code>{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n</code></pre></p>"},{"location":"configuration/#custom-llm-providers","title":"Custom LLM providers","text":"<p>It's possible to configure ECA to be aware of custom LLM providers if they follow a API schema similar to currently supported ones (openai, anthropic), example for a custom hosted litellm server:</p> <pre><code>{\n  \"customProviders\": {\n    \"my-company\": {\n       \"api\": \"openai\",\n       \"urlEnv\": \"MY_COMPANY_API_URL\", // or \"url\": \"https://litellm.my-company.com\",\n       \"keyEnv\": \"MY_COMPANY_API_KEY\", // or \"key\": \"123\",\n       \"models\": [\"gpt-4.1\", \"deepseek-r1\"],\n       \"defaultModel\": \"deepseek-r1\"\n    }\n  }\n}\n</code></pre> <p>With that, ECA will include in the known models something like: <code>my-company/gpt-4.1</code>, <code>my-company/deepseek-r1</code>.</p>"},{"location":"configuration/#all-configs","title":"All configs","text":""},{"location":"configuration/#schema","title":"Schema","text":"<pre><code>interface Config {\n    openaiApiKey?: string;\n    anthropicApiKey?: string;\n    rules: [{path: string;}];\n    nativeTools: {\n        filesystem: {enabled: boolean};\n         shell: {enabled: boolean;\n                 excludeCommands: string[]};\n    };\n    disabledTools: string[],\n    toolCall?: {\n      manualApproval?: boolean,\n    };\n    mcpTimeoutSeconds: number;\n    mcpServers: {[key: string]: {\n        command: string;\n        args?: string[];\n        disabled?: boolean; \n    }};\n    customProviders: {[key: string]: {\n        api: 'openai' | 'anthropic';\n        models: string[];\n        defaultModel?: string;\n        url?: string;\n        urlEnv?: string;\n        key?: string;\n        keyEnv?: string;\n    }};\n    ollama?: {\n        host: string;\n        port: string;\n        useTools: boolean;\n        think: boolean;\n    };\n    chat?: {\n        welcomeMessage: string;\n    };\n    index?: {\n        ignoreFiles: [{\n            type: string;\n        }];\n    };\n}\n</code></pre>"},{"location":"configuration/#default-values","title":"Default values","text":"<pre><code>{\n  \"openaiApiKey\" : null,\n  \"anthropicApiKey\" : null,\n  \"rules\" : [],\n  \"nativeTools\": {\"filesystem\": {\"enabled\": true},\n                  \"shell\": {\"enabled\": true, \n                            \"excludeCommands\": []}},\n  \"disabledTools\": [],\n  \"toolCall\": {\n    \"manualApproval\": false,\n  },\n  \"mcpTimeoutSeconds\" : 10,\n  \"mcpServers\" : [],\n  \"customProviders\": {},\n  \"ollama\" : {\n    \"host\" : \"http://localhost\",\n    \"port\" : 11434,\n    \"useTools\": true,\n    \"think\": true\n  },\n  \"chat\" : {\n    \"welcomeMessage\" : \"Welcome to ECA! What you have in mind?\\n\\n\"\n  },\n  \"index\" : {\n    \"ignoreFiles\" : [ {\n      \"type\" : \"gitignore\"\n    } ]\n  }\n}\n</code></pre>"},{"location":"development/","title":"ECA Development","text":""},{"location":"development/#project-structure","title":"Project structure","text":"<p>The ECA codebase follows a pragmatic layered layout that separates concerns clearly so that you can jump straight to the part you need to change.</p>"},{"location":"development/#files-overview","title":"Files overview","text":"Path Responsibility <code>bb.edn</code> Babashka tasks (e.g. <code>bb test</code>, <code>bb debug-cli</code>) for local workflows and CI, the main entrypoint for most tasks. <code>deps.edn</code> Clojure dependency coordinates and aliases used by the JVM build and the native GraalVM image. <code>docs/</code> Markdown documentation shown at https://eca.dev <code>src/eca/config.clj</code> Centralized place to get ECA configs from multiple places. <code>src/eca/logger.clj</code> Logger interface to log to stderr. <code>src/eca/shared.clj</code> shared utility fns to whole project. <code>src/eca/db.clj</code> Simple in-memory KV store that backs sessions/MCP, all in-memory statue lives here. <code>src/eca/llm_api.clj</code> Public fa\u00e7ade used by features to call an LLM. <code>src/eca/llm_providers/</code> Vendor adapters (<code>openai.clj</code>, <code>anthropic.clj</code>, <code>ollama.clj</code>). <code>src/eca/llm_util.clj</code> Token counting, chunking, rate-limit helpers. <code>src/eca/features/</code> High-level capabilities exposed to the editor \u251c\u2500 <code>chat.clj</code> Streaming chat orchestration &amp; tool-call pipeline. \u251c\u2500 <code>prompt.clj</code> Prompt templates and variable interpolation. \u251c\u2500 <code>index.clj</code> Embedding &amp; retrieval-augmented generation helpers. \u251c\u2500 <code>rules.clj</code> Guards that enforce user-defined project rules. \u251c\u2500 <code>tools.clj</code> Registry of built-in tool descriptors (run, approve\u2026). \u2514\u2500 <code>tools/</code> Implementation of side-effectful tools: \u2500\u2500\u251c\u2500 <code>filesystem.clj</code> read/write/edit helpers\u2003 \u2500\u2500\u251c\u2500 <code>shell.clj</code> runs user-approved shell commands\u2003 \u2500\u2500\u251c\u2500 <code>mcp.clj</code> Multi-Command Plan supervisor\u2003 \u2500\u2500\u2514\u2500 <code>util.clj</code> misc helpers shared by tools. <code>src/eca/messenger.clj</code> To send back to client requests/notifications over stdio. <code>src/eca/handlers.clj</code> Entrypoint for all features. <code>src/eca/server.clj</code> stdio entry point; wires everything together via <code>lsp4clj</code>. <code>src/eca/main.clj</code> The CLI interface. <code>src/eca/nrepl.clj</code> Starts an nREPL when <code>:debug</code> flag is passed. <p>Together these files implement the request flow: </p> <p><code>client/editor</code> \u2192 <code>stdin JSON-RPC</code> \u2192 <code>handlers</code> \u2192 <code>features</code> \u2192 <code>llm_api</code> \u2192 <code>llm_provider</code> \u2192 results streamed back.</p> <p>With this map you can usually answer:</p> <ul> <li>\"Where does request X enter the system?\" \u2013 look in <code>handlers.clj</code>.</li> <li>\"How is tool Y executed?\" \u2013 see <code>src/eca/features/tools/&lt;y&gt;.clj</code>.</li> <li>\"How do we talk to provider Z?\" \u2013 adapter under <code>llm_providers/</code>.</li> </ul>"},{"location":"development/#tests","title":"Tests","text":"<p>Run with <code>bb test</code> or run test via Clojure REPL. CI will run the same task.</p>"},{"location":"development/#coding","title":"Coding","text":"<p>There are several ways of finding and fixing a bug or implementing a new feature:</p> <ul> <li>Create a test for your bug/feature, then implement the code following the test (TDD).</li> <li>Build a local <code>eca</code> JVM embedded binary using <code>bb debug-cli</code> (requires <code>babashka</code>), and test it manually in your client pointing to it. After started, you can connect to the nrepl port mentioned in eca stderr buffer, do you changes, evaluate and it will be affected on the running eca.</li> <li>Using a debug binary you can check eca's stderr buffer and look for a nrepl port, and connect to the REPL, make changes to the running eca process (really handy).</li> </ul>"},{"location":"development/#supporting-a-new-editor","title":"Supporting a new editor","text":"<p>When supporting a new editor, it's important to keep UX consistency across editors, check how other editors done or ask for help.</p> <p>This step-by-step feature implementation help track progress and next steps:</p> <pre><code>- [ ] Create the plugin/extension repo (editor-code-assistant/eca-&lt;editor&gt; would be ideal), ask maintainers for permission.\n- Server\n  - Manage ECA server process.\n    - [ ] Automatic download of latest server.\n    - [ ] Allow user specify server path/args.\n    - [ ] Commands for Start/stop server from editor.\n    - [ ] Show server status (modeline, bottom of editor, etc).\n  - [ ] JSONRPC communication with eca server process via stdin/stdout sending/receiving requests and notifications, check [protocol](./protocol.md).\n  - [ ] Allow check eca server process stderr for debugging/logs.\n  - [ ] Support `initialize` and `initialized` methods.\n  - [ ] Support `exit` and `shutdown` methods.\n- Chat\n  - [ ] Oepn chat window\n  - [ ] Send user messages via `chat/prompt` request.\n  - [ ] Clear chat and Reset chat.\n  - [ ] Support receive chat contents via `chat/contentReceived` notification.\n  - [ ] Present and allow user change behaviors and models returned from `initialize` request.\n  - [ ] Present and add contexts via `chat/queryContext` request\n  - [ ] Support tools contents: run/approval/reject via `chat/toolCallApprove` or `chat/toolCallReject`.\n  - [ ] Support reason/thoughts content blocks.\n  - [ ] Show MCPs summary (running, failed, pending).\n  - [ ] Support chat commands (`/`) auto completion, querying via `chat/queryCommands`.\n  - [ ] Show usage (costs/tokens) from usage content blocks.\n  - [ ] keybindings: navigate through chat blocks/messages, clear chat.\n- MCP\n  - [ ] Open MCP details window\n  - [ ] Receive MCP server updates and update chat and mcp-details ux.\n- [ ] Basic plugin/extension documentation\n</code></pre> <p>Create a issue to help track the effort copying and pasting these check box to help track progress, example.</p> <p>Please provide feedback of the dificulties implementing your server, especially missing docs, to make next integrations smoother!</p>"},{"location":"features/","title":"Features","text":""},{"location":"features/#chat","title":"Chat","text":"<p>Chat is the main feature of ECA, allowing LLM to behave like a chat, answering questions, or agent, making changes using tools.</p>"},{"location":"features/#tools","title":"Tools","text":"<p>ECA leverage tools to give more power to the LLM, this is the best way to make LLMs have more context about your codebase and behave like an agent. It supports both MCP server tools + ECA native tools, for more details, check configuration.</p>"},{"location":"features/#native-tools","title":"Native tools","text":"<p>ECA support built-in tools to avoid user extra installation and configuration, these tools are always included on models requests that support tools and can be disabled/configured via config <code>nativeTools</code>.</p> <p>Some native tools like <code>filesystem</code> have MCP alternatives, but ECA having them built-in avoid the need to external dependencies like npx.</p>"},{"location":"features/#filesystem","title":"Filesystem","text":"<p>Provides access to filesystem under workspace root, listing and reading files and directories a subset of official MCP filesystem, important for agentic operations, without the need to support NPM or other tools.</p> <ul> <li><code>eca_read_file</code>: read a file content.</li> <li><code>eca_write_file</code>: write content to file.</li> <li><code>eca_move_file</code>: move/rename a file.</li> <li><code>eca_list_directory</code>: list a directory.</li> <li><code>eca_search_files</code>: search in a path for files matching a pattern.</li> <li><code>eca_grep</code>: ripgrep/grep for paths with specified content.</li> <li><code>eca_replace_in_file</code>: replace a text with another one in file.</li> </ul>"},{"location":"features/#shell","title":"Shell","text":"<p>Provides access to run shell commands, useful to run build tools, tests, and other common commands, supports exclude/include commands. </p> <ul> <li><code>eca_shell_command</code>: run shell command. Supports configs to exclude commands via <code>:nativeTools :shell :excludeCommands</code>.</li> </ul>"},{"location":"features/#contexts","title":"Contexts","text":"<p>User can include contexts to the chat, which can help LLM generate output with better quality. Here are the current supported contexts types:</p> <ul> <li><code>file</code>: a file in the workspace, server will pass its content to LLM.</li> <li><code>directory</code>: a directory in the workspace, server will read all file contexts and pass to LLM.</li> <li><code>repoMap</code>: a summary view of workspaces files and folders, server will calculate this and pass to LLM. Currently, the repo-map includes only the file paths in git.</li> </ul>"},{"location":"features/#commands","title":"Commands","text":"<p>Eca supports commands that usually arer triggered via <code>/</code> in the chat, completing in the chat will show the known commands which include ECA commands and MCP prompts.</p>"},{"location":"features/#completion","title":"Completion","text":"<p>Soon</p>"},{"location":"features/#edit","title":"Edit","text":"<p>Soon</p>"},{"location":"installation/","title":"Installation","text":"<p>Eca is written in Clojure and compiled into a native binary via graalvm. You can download the native binaries from Github Releases or use the install script for convenience:</p> <p>Stable release:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install)\n</code></pre> <p>Or if facing issues with command above: <pre><code>curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install | sudo bash\n</code></pre></p> <p>nightly build:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install) --version nightly --dir ~/\n</code></pre>"},{"location":"models/","title":"Models","text":""},{"location":"models/#built-in-providers-and-capabilities","title":"Built-in providers and capabilities","text":"model MCP / tools MCP prompts thinking/reasioning prompt caching web_search OpenAI \u221a \u221a X X \u221a Anthropic \u221a \u221a X \u221a \u221a Ollama \u221a \u221a X X X"},{"location":"models/#openai","title":"OpenAI","text":"<ul> <li>o4-mini</li> <li>o3</li> <li>gpt-4.1</li> </ul>"},{"location":"models/#anthropic","title":"Anthropic","text":"<ul> <li>claude-sonnet-4-0</li> <li>claude-opus-4-0</li> <li>claude-3-5-haiku-latest</li> </ul>"},{"location":"models/#ollama","title":"Ollama","text":"<ul> <li>any local ollama model</li> </ul>"},{"location":"models/#custom-providers","title":"Custom providers","text":"<p>ECA support configure extra LLM providers via <code>customProviders</code> config, for more details check configuration.</p>"},{"location":"protocol/","title":"ECA Protocol","text":"<p>The ECA (Editor Code Assistant) protocol is JSON-RPC 2.0-based protocol heavily insipired by the LSP (Language Server Protocol), that enables communication between multiple code editors/IDEs and ECA process (server), which will interact with multiple LLMs. It follows similar patterns to the LSP but is specifically designed for AI code assistance features.</p> <p>Key characteristics: - Provides a protocol standard so different editors can use the same language to offer AI features. - Supports bidirectional communication (client to server and server to client) - Handles both synchronous requests and asynchronous notifications - Includes built-in support for streaming responses - Provides structured error handling</p>"},{"location":"protocol/#base-protocol","title":"Base Protocol","text":"<p>The base protocol consists of a header and a content part (comparable to HTTP). The header and content part are separated by a <code>\\r\\n</code>.</p>"},{"location":"protocol/#header-part","title":"Header Part","text":"<p>The header part consists of header fields. Each header field is comprised of a name and a value, separated by <code>:</code> (a colon and a space). The structure of header fields conforms to the HTTP semantic. Each header field is terminated by <code>\\r\\n</code>. Considering the last header field and the overall header itself are each terminated with <code>\\r\\n</code>, and that at least one header is mandatory, this means that two <code>\\r\\n</code> sequences always immediately precede the content part of a message.</p> <p>Currently the following header fields are supported:</p> Header Field Name Value Type Description Content-Length number The length of the content part in bytes. This header is required. Content-Type string The mime type of the content part. Defaults to application/vscode-jsonrpc; charset=utf-8 {: .table .table-bordered .table-responsive} <p>The header part is encoded using the 'ascii' encoding. This includes the <code>\\r\\n</code> separating the header and content part.</p>"},{"location":"protocol/#content-part","title":"Content Part","text":"<p>Contains the actual content of the message. The content part of a message uses JSON-RPC 2.0 to describe requests, responses and notifications. The content part is encoded using the charset provided in the Content-Type field. It defaults to <code>utf-8</code>, which is the only encoding supported right now. If a server or client receives a header with a different encoding than <code>utf-8</code> it should respond with an error.</p>"},{"location":"protocol/#example","title":"Example:","text":"<pre><code>Content-Length: ...\\r\\n\n\\r\\n\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"initialize\",\n    \"params\": {\n        ...\n    }\n}\n</code></pre>"},{"location":"protocol/#lifecycle-messages","title":"Lifecycle Messages","text":"<p>The protocol defines a set of lifecycle messages that manage the connection and state between the client (editor) and server (code assistant):</p>"},{"location":"protocol/#initialize","title":"Initialize (\u21a9\ufe0f)","text":"<p>The first request sent from client to server. This message: - Establishes the connection - Allows the server to index the project - Enables capability negotiation - Sets up the workspace context</p> <p>Request:</p> <ul> <li>method: <code>initialize</code></li> <li>params: <code>InitializeParams</code> defined as follows:</li> </ul> <pre><code>interface InitializeParams {\n    /**\n     * The process Id of the parent process that started the server. Is null if\n     * the process has not been started by another process. If the parent\n     * process is not alive then the server should exit (see exit notification)\n     * its process.\n     */\n     processId: integer | null;\n\n     /**\n     * Information about the client\n     */\n    clientInfo?: {\n        /**\n         * The name of the client as defined by the client.\n         */\n        name: string;\n\n        /**\n         * The client's version as defined by the client.\n         */\n        version?: string;\n    };\n\n    /**\n     * User provided initialization options.\n     */\n    initializationOptions?: {\n        /*\n         * The chat behavior.\n         */\n         chatBehavior?: ChatBehavior;\n    };\n\n    /**\n     * The capabilities provided by the client (editor or tool)\n     */\n    capabilities: ClientCapabilities;\n\n    /**\n     * The workspace folders configured in the client when the server starts.\n     * If client doesn\u00b4t support multiple projects, it should send a single \n     * workspaceFolder with the project root.\n     */\n    workspaceFolders: WorkspaceFolder[];\n}\n\ninterface WorkspaceFolder {\n    /**\n     * The associated URI for this workspace folder.\n     */\n    uri: string;\n\n    /**\n     * The name of the workspace folder. Used to refer to this folder in the user interface.\n     */\n    name: string;\n}\n\ninterface ClientCapabilities {\n    codeAssistant?: {\n        chat?: boolean;\n        doc?: boolean;\n        edit?: boolean;\n        fix?: boolean;\n    }\n}\n\ntype ChatBehavior = 'agent' | 'chat';\n</code></pre> <p>Response:</p> <pre><code>interface InitializeResponse {\n\n    /*\n     * The models supported by the server.\n     */\n    models: ChatModel[];\n\n    /*\n     * Default model used by server.\n     */\n    chatDefaultModel: ChatModel;\n\n    /*\n     * The chat behaviors available.\n     */\n    chatBehaviors: ChatBehavior[];\n\n    /*\n     * Default chat behavior used by server.\n     */\n    chatDefaultBehavior: ChatBehavior;\n\n    /*\n     * The chat welcome message when chat is cleared or in a new state.\n     */\n    chatWelcomeMessage: string;\n}\n</code></pre>"},{"location":"protocol/#initialized","title":"Initialized (\u27a1\ufe0f)","text":"<p>A notification sent from the client to the server after receiving the initialize response. This message: - Confirms that the client is ready to receive requests - Signals that the server can start sending notifications - Indicates that the workspace is fully loaded</p> <p>Notification:</p> <ul> <li>method: <code>initialized</code></li> <li>params: <code>InitializedParams</code> defined as follows:</li> </ul> <pre><code>interface InitializedParams {}\n</code></pre>"},{"location":"protocol/#shutdown","title":"Shutdown (\u21a9\ufe0f)","text":"<p>A request sent from the client to the server to gracefully shut down the connection. This message: - Allows the server to clean up resources - Ensures all pending operations are completed - Prepares for a clean disconnection</p> <p>Request:</p> <ul> <li>method: <code>shutdown</code></li> <li>params: none</li> </ul> <p>Response:</p> <ul> <li>result: null</li> <li>error: code and message set in case an exception happens during shutdown request.</li> </ul>"},{"location":"protocol/#exit","title":"Exit (\u27a1\ufe0f)","text":"<p>A notification sent from the client to the server to terminate the connection. This message: - Should be sent after a shutdown request - Signals the server to exit its process - Ensures all resources are released</p> <p>Notification:</p> <ul> <li>method: <code>exit</code></li> <li>params: none </li> </ul>"},{"location":"protocol/#code-assistant-features","title":"Code Assistant Features","text":""},{"location":"protocol/#chat-prompt","title":"Chat Prompt (\u21a9\ufe0f)","text":"<p>A request sent from client to server, starting or continuing a chat in natural language as an agent. Used for broader questions or continuous discussion of project/files.</p> <p>Request: </p> <ul> <li>method: <code>chat/prompt</code></li> <li>params: <code>ChatPromptParams</code> defined as follows:</li> </ul> <pre><code>interface ChatPromptParams {\n    /**\n     * The chat session identifier. If not provided, a new chat session will be created.\n     */\n    chatId?: string;\n\n    /**\n     * This message unique identifier used to match with next async messages.\n     */\n    requestId: string;\n\n    /**\n     * The message from the user in native language\n     */\n    message: string;\n\n    /**\n     * Specifies the AI model to be used for chat responses.\n     * Different models may have different capabilities, response styles,\n     * and performance characteristics.\n     */\n    model?: ChatModel;\n\n    /**\n     * The chat behavior used by server to handle chat communication and actions.\n     */\n    behavior?: ChatBehavior;\n\n    /**\n     * Optional contexts about the current workspace.\n     * Can include multiple different types of context.\n     */\n    contexts?: ChatContext[];\n}\n\n/**\n * The currently supported models.\n */\ntype ChatModel = \n    | 'o4-mini'\n    | 'o3'\n    | 'gpt-4.1'\n    | 'claude-sonnet-4-0'\n    | 'claude-opus-4-0'\n    | 'claude-3-5-haiku-latest'\n    OllamaRunningModel;\n\n/**\n * Ollama running models available locally.\n */\ntype OllamaRunningModel = string\n\ntype ChatContext = FileContext | DirectoryContext | WebContext | RepoMapContext;\n\n/**\n * Context related to a file in the workspace\n */\ninterface FileContext {\n    type: 'file';\n    /**\n     * Path to the file\n     */\n    path: string;\n\n    /**\n     * Range of lines to retrive from file, if nil consider whole file.\n     */\n    linesRange?: {\n        start: number;\n        end: number;\n    }\n}\n\n/**\n * Context related to a directory in the workspace\n */\ninterface DirectoryContext {\n    type: 'directory';\n    /**\n     * Path to the directory\n     */\n    path: string;\n}\n\n/**\n * Context related to web content\n */\ninterface WebContext {\n    type: 'web';\n    /**\n     * URL of the web content\n     */\n    url: string;\n}\n\n/**\n * Context about the workspaces repo-map, automatically calculated by server.\n * Clients should include this to chat by default but users may want exclude \n * this context to reduce context size if needed.\n */\ninterface RepoMapContext {\n    type: 'repoMap'; \n }\n</code></pre> <p>Response:</p> <pre><code>interface ChatPromptResponse {\n    /**\n     * Unique identifier for this chat session\n     */\n    chatId: string;\n\n    /*\n     * The model used for this chat request.\n     */\n    model: ChatModel;\n\n    status: 'success';\n}\n</code></pre>"},{"location":"protocol/#chat-content-received","title":"Chat Content Received (\u2b05\ufe0f)","text":"<p>A server notification with a new content from the LLM.</p> <p>Notification: </p> <ul> <li>method: <code>chat/contentReceived</code></li> <li>params: <code>ChatContentReceivedParams</code> defined as follows:</li> </ul> <pre><code>interface ChatContentReceivedParams {\n    /**\n     * The chat session identifier this content belongs to\n     */\n    chatId: string;\n\n    /**\n     * The content received from the LLM\n     */\n    content: ChatContent;\n\n    /**\n     * The owner of this content.\n     */\n    role: 'user' | 'system' | 'assistant';\n}\n\n/**\n * Different types of content that can be received from the LLM\n */\ntype ChatContent = \n    | TextContent \n    | URLContent \n    | ProgressContent \n    | UsageContent\n    | ReasonStartedContent \n    | ReasonTextContent \n    | ReasonFinishedContent \n    | ToolCallPrepareContent\n    | ToolCallRunContent\n    | ToolCalledContent;\n\n/**\n * Simple text message from the LLM\n */\ninterface TextContent {\n    type: 'text';\n    /**\n     * The text content\n     */\n    text: string;\n}\n\n/**\n * A reason started from the LLM\n *\n */\ninterface ReasonStartedContent {\n    type: 'reasonStarted';\n\n    /**\n     * The id of this reason\n     */\n    id: string; \n}\n\n/**\n * A reason text from the LLM\n *\n */\ninterface ReasonTextContent {\n    type: 'reasonText';\n\n    /**\n     * The id of a started reason\n     */\n    id: string;\n\n    /**\n     * The text content of the reasoning\n     */\n    text: string;\n}\n\n/**\n * A reason finished from the LLM\n *\n */\ninterface ReasonFinishedContent {\n    type: 'reasonFinished';\n\n    /**\n     * The id of this reason\n     */\n    id: string; \n}\n\n/**\n * URL content message from the LLM\n */\ninterface URLContent {\n    type: 'url';\n\n    /**\n     * The URL title\n     */\n    title: string;\n\n    /**\n     * The URL link\n     */\n    url: string;\n}\n\n/**\n * Details about the chat's usage, like used tokens and costs.\n */\ninterface UsageContent {\n    type: 'usage';\n\n    /*\n     * Number of tokens sent on previous prompt including all context used by ECA.\n     */\n    messageInputTokens: number;\n\n    /*\n     * Number of tokens received from LLm in last prompt.\n     */\n    messageOutputTokens: number;\n\n    /**\n     * The total input + output tokens of the whole chat session so far.\n     */\n    sessionTokens: number;\n\n    /**\n     * The cost of the last sent message summing input + output tokens.\n     */\n    messageCost?: string; \n\n    /**\n     * The cost of the whole chat session so far.\n     */\n    sessionCost?: string;\n}\n\n/**\n * Tool call that LLM is preparing to execute.\n */\ninterface ToolCallPrepareContent {\n    type: 'toolCallPrepare';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /*\n     * Argument text of this tool call\n     */\n    argumentsText: string;\n\n    /**\n     * Whether this call requires manual approval from the user.\n     */\n    manualApproval: boolean;\n}\n\n/**\n * Tool call final request that LLM may trigger.\n */\ninterface ToolCallRunContent {\n    type: 'toolCallRun';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * Whether this call requires manual approval from the user.\n     */\n    manualApproval: boolean;\n}\n\n/**\n * Tool call result that LLM trigerred and was executed already.\n */\ninterface ToolCalledContent {\n    type: 'toolCalled';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: string[];\n\n    /**\n     * Whether it was a error\n     */\n    error: boolean;\n\n    /**\n     * the result of the tool call.\n     */\n    outputs: [{\n        /*\n         * The type of this output\n         */\n        type: 'text';\n\n        /**\n         * The content of this output\n         */\n        content: string; \n    }];\n}\n\ninterface ToolCallRejected {\n    type: 'toolCallRejected';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * The reason why this tool call was rejected\n     */\n    reason: 'user';\n}\n\ntype ToolCallOrigin = 'mcp' | 'native';\n</code></pre>"},{"location":"protocol/#chat-approve-tool-call","title":"Chat approve tool call (\u27a1\ufe0f)","text":"<p>A client notification for server to approve a waiting tool call. This will execute the tool call and continue the LLM chat loop.</p> <p>Notification:</p> <ul> <li>method: <code>chat/toolCallApprove</code></li> <li>params: <code>ChatToolCallApproveParams</code> defined as follows:</li> </ul> <pre><code>interface ChatToolCallApproveParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The tool call identifier to approve.\n     */\n    toolCallId: string; \n}\n</code></pre>"},{"location":"protocol/#chat-reject-tool-call","title":"Chat reject tool call (\u27a1\ufe0f)","text":"<p>A client notification for server to reject a waiting tool call. This will not execute the tool call and return to the LLM chat loop.</p> <p>Notification:</p> <ul> <li>method: <code>chat/toolCallReject</code></li> <li>params: <code>ChatToolCallRejectParams</code> defined as follows:</li> </ul> <pre><code>interface ChatToolCallRejectParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The tool call identifier to reject.\n     */\n    toolCallId: string; \n}\n</code></pre>"},{"location":"protocol/#chat-query-context","title":"Chat Query Context (\u21a9\ufe0f)","text":"<p>A request sent from client to server, querying for all the available contexts for user add to prompt calls.</p> <p>Request: </p> <ul> <li>method: <code>chat/queryContext</code></li> <li>params: <code>ChatQueryContextParams</code> defined as follows:</li> </ul> <pre><code>interface ChatQueryContextParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The query to filter results, blank string returns all available contexts.\n     */\n    query: string;\n\n    /**\n     * The already considered contexts.\n     */\n    contexts: ChatContext[];\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatQueryContextResponse {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The returned available contexts.\n     */\n    contexts: ChatContext[];\n}\n</code></pre>"},{"location":"protocol/#chat-query-commands","title":"Chat Query Commands (\u21a9\ufe0f)","text":"<p>A request sent from client to server, querying for all the available commands for user to call. Commands are multiple possible actions like MCP prompts, doctor, costs. Usually the  UX follows <code>/&lt;command&gt;</code> to spawn a command.</p> <p>Request: </p> <ul> <li>method: <code>chat/queryCommands</code></li> <li>params: <code>ChatQueryCommandsParams</code> defined as follows:</li> </ul> <pre><code>interface ChatQueryCommandsParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The query to filter results, blank string returns all available commands.\n     */\n    query: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatQueryCommandsResponse {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The returned available Commands.\n     */\n    commands: ChatCommand[];\n}\n\ninterface ChatCommand {\n    /**\n     * The name of the command.\n     */\n    name: string;\n\n    /**\n     * The description of the command.\n     */\n    description: string;\n\n    /**\n     * The type of this command\n     */\n    type: 'mcp-prompt' | 'native';\n\n    /**\n     * The arguments of the command.\n     */\n    arguments: [{\n       name: string;\n       description?: string;\n       required: boolean; \n    }];\n}\n</code></pre>"},{"location":"protocol/#chat-stop-prompt","title":"Chat stop prompt (\u27a1\ufe0f)","text":"<p>A client notification for server to stop the current chat prompt with LLM if running. This will stop LLM loops or ignore subsequent LLM responses so other prompts can be trigerred.</p> <p>Notification:</p> <ul> <li>method: <code>chat/promptStop</code></li> <li>params: <code>ChatPromptStopParams</code> defined as follows:</li> </ul> <pre><code>interface ChatPromptStopParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n}\n</code></pre>"},{"location":"protocol/#chat-delete","title":"Chat delete (\u21a9\ufe0f)","text":"<p>A client request to delete a existing chat, removing all previous messages and used tokens/costs from memory, good for reduce context or start a new clean chat. After response, clients should reset chat UI to a clean state.</p> <p>Request: </p> <ul> <li>method: <code>chat/delete</code></li> <li>params: <code>ChatDeleteParams</code> defined as follows:</li> </ul> <pre><code>interface ChatDeleteParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatDeleteResponse {}\n</code></pre>"},{"location":"protocol/#completion","title":"Completion (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#edit","title":"Edit (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#configuration","title":"Configuration","text":""},{"location":"protocol/#tool-updated","title":"Tool updated (\u2b05\ufe0f)","text":"<p>A server notification about a tool status update like a MCP or native tool. This is useful for clients present to user the list of configured tools/MCPs, their status and available tools and actions.</p> <p>Notification: </p> <ul> <li>method: <code>tool/serverUpdated</code></li> <li>params: <code>ToolServerUpdatedParams</code> defined as follows:</li> </ul> <pre><code>type ToolServerUpdatedParams = EcaServerUpdatedParams | MCPServerUpdatedParams;\n\ninterface EcaServerUpdatedParams {\n    type: 'native';\n\n    name: 'ECA';\n\n    status: 'running';\n\n    /**\n     * The built-in tools supported by eca.\n     */\n    tools: ServerTool[];\n}\n\ninterface MCPServerUpdatedParams {\n    type: 'mcp';\n\n    /**\n     * The server name.\n     */\n    name: string;\n\n    /**\n     * The command to start this server.\n     */\n    command: string;\n\n    /**\n     * The arguments to start this server.\n     */\n    args: string[];\n\n    /**\n     * The status of the server.\n     */\n    status: 'running' | 'starting' | 'stopped' | 'failed' | 'disabled';\n\n    /**\n     * The tools supported by this mcp server if not disabled.\n     */\n    tools?: ServerTool[];\n}\n\ninterface ServerTool {\n    /**\n     * The server tool name.\n     */\n    name: string;\n\n    /**\n     * The server tool description.\n     */\n    description: string;\n\n    /**\n     * The server tool parameters.\n     */\n    parameters: any; \n\n    /**\n     * Whther this tool is disabled.\n     */\n    disabled?: boolean;\n}\n</code></pre>"},{"location":"protocol/#stop-mcp-server","title":"Stop MCP server (\u27a1\ufe0f)","text":"<p>A client notification for server to stop a MCP server, stopping the process. Updates its status via <code>tool/serverUpdated</code> notification.</p> <p>Notification:</p> <ul> <li>method: <code>mcp/stopServer</code></li> <li>params: <code>MCPStopServerParams</code> defined as follows:</li> </ul> <pre><code>interface MCPStopServerParams {\n    /**\n     * The MCP server name.\n     */\n    name: string;\n}\n</code></pre>"},{"location":"protocol/#start-mcp-server","title":"Start MCP server (\u27a1\ufe0f)","text":"<p>A client notification for server to start a stopped MCP server, starting the process again. Updates its status via <code>tool/serverUpdated</code> notification.</p> <p>Notification:</p> <ul> <li>method: <code>mcp/startServer</code></li> <li>params: <code>MCPStartServerParams</code> defined as follows:</li> </ul> <pre><code>interface MCPStartServerParams {\n    /**\n     * The server name.\n     */\n    name: string;\n}\n</code></pre>"},{"location":"protocol/#add-mcp","title":"Add MCP (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#general-features","title":"General features","text":""},{"location":"protocol/#showmessage","title":"showMessage (\u2b05\ufe0f)","text":"<p>A notification from server telling client to present a message to user.</p> <p>Request: </p> <ul> <li>method: <code>$/showMessage</code></li> <li>params: <code>ShowMessageParams</code> defined as follows:</li> </ul> <pre><code>interface ShowMessageParams {\n    /**\n     * The message type. See {@link MessageType}.\n    */\n    type: MessageType;\n\n    /**\n     * The actual message.\n     */\n    message: string;\n}\n\nexport type MessageType = 'error' | 'warning' | 'info';\n</code></pre>"}]}